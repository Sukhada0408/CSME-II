{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSME Bonus Point Assignment I Part A\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.1, released 2021-01-04</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A1 - Neural Network Class\n",
    "In this task you will implement the building blocks of a simple neural network class.\n",
    "\n",
    "For simplicity's sake, this class will only allow feed forward networks.\n",
    "All individual operations, including activation functions, will be represented as layers in this model.\n",
    "The abstract `Layer` interface defines three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the layer.\n",
    "        For convenience, input and output are stored in the layer. \n",
    "\n",
    "        Args:\n",
    "            x: Input for this layer. Shape is (batch_size, num_inputs).\n",
    "\n",
    "        Returns:\n",
    "            x: Output of this layer. Shape is (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Backward pass of the layer.\n",
    "        The incoming gradients are stored in the layer. \n",
    "\n",
    "        Args:\n",
    "            gradient: Incoming gradient from the next layer. Shape is (batch_size, num_outputs).\n",
    "\n",
    "        Returns:\n",
    "            gradient: Gradient passed to previous layer. Shape is (batch_size, num_inputs).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, learn_rate):\n",
    "        \"\"\"Perform weight update based on previously stored input and gradients.\n",
    "\n",
    "        Args:\n",
    "            learn_rate: Learn rate to use for the update.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, seed=None):\n",
    "        \"\"\"Initialize the layer with random weights.\"\"\"\n",
    "        # Initialize weights with the He initializer\n",
    "        rnd = torch.random.RandomState(seed).randn(input_dim, output_dim)\n",
    "        self.w = rnd * (2 / input_dim) \n",
    "        \n",
    "        # Initialize bias with zeros\n",
    "        self.b = np.zeros(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the layer.\"\"\"\n",
    "        self.input = x # Store input\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Compute output\n",
    "        \n",
    "        x = x@self.w + self.b\n",
    "        \n",
    "        # ********************\n",
    "        \n",
    "        self.output = x # Store output\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Backward pass of the layer.\"\"\"\n",
    "        self.gradient = gradient # Store incoming gradient\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Apply transfer gradient\n",
    "        \n",
    "        gradient = gradient@self.w.T #local_gradient*upstream_gradient \n",
    "        \n",
    "        # ********************\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def update(self, learn_rate):\n",
    "        \"\"\"Perform weight update\"\"\"\n",
    "        # ********************\n",
    "        # TODO Update weights and bias\n",
    "        \n",
    "        self.w = self.w - learn_rate*(self.input.T@self.gradient)\n",
    "        self.b = self.b - learn_rate*np.sum(self.gradient, axis=0) \n",
    "        \n",
    "        \n",
    "        # ********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.random' has no attribute 'RandomState'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m _test_layer \u001b[38;5;241m=\u001b[39m \u001b[43mLinearLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test forward pass\u001b[39;00m\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_array_almost_equal(_test_layer\u001b[38;5;241m.\u001b[39mforward(np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))), [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.38156401\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.01880618\u001b[39m,  \u001b[38;5;241m0.98566644\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.88387442\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.41440743\u001b[39m]])\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mLinearLayer.__init__\u001b[1;34m(self, input_dim, output_dim, seed)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Initialize the layer with random weights.\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize weights with the He initializer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m rnd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomState\u001b[49m(seed)\u001b[38;5;241m.\u001b[39mrandn(input_dim, output_dim)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m rnd \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m input_dim) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize bias with zeros\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.random' has no attribute 'RandomState'"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "_test_layer = LinearLayer(4, 5, seed=42)\n",
    "\n",
    "# Test forward pass\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward(np.ones((1, 4))), [[-0.38156401, -0.01880618,  0.98566644, -0.88387442, -1.41440743]])\n",
    "\n",
    "# Test backward pass\n",
    "np.testing.assert_array_almost_equal(_test_layer.backward(np.ones((1, 5))), [[1.147507,  1.092798, -2.162692, -1.7906 ]])\n",
    "\n",
    "# Test update\n",
    "_test_layer.update(0.1)\n",
    "np.testing.assert_array_almost_equal(_test_layer.w, [[ 0.148357, -0.169132,  0.223844,  0.661515, -0.217077], [-0.217068,  0.689606,  0.283717, -0.334737,  0.17128 ], [-0.331709, -0.332865,  0.020981, -1.05664 , -0.962459], [-0.381144, -0.606416,  0.057124, -0.554012, -0.806152]])\n",
    "np.testing.assert_array_almost_equal(_test_layer.b, [-0.1, -0.1, -0.1, -0.1, -0.1])\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Implementing ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(Layer):      \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the ReLU layer.\"\"\"\n",
    "        self.input = x # Store input\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Compute output\n",
    "        \n",
    "        x = x*(x > 0)\n",
    "        \n",
    "        \n",
    "        # ********************\n",
    "        \n",
    "        self.output = x # Store output\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Backward pass of the ReLU layer.\"\"\"\n",
    "        self.gradient = gradient # Store incoming gradient\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Apply transfer gradient\n",
    "        \n",
    "        gradient = np.multiply(self.gradient,((self.input > 0)*1))\n",
    "        \n",
    "        # ********************\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "_test_layer = ReLULayer()\n",
    "\n",
    "# Test forward pass\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward(np.arange(5)-2), [0, 0,  0, 1, 2])\n",
    "\n",
    "# Test backward pass\n",
    "assert np.array_equal(_test_layer.backward(np.ones(5)), [0, 0, 0, 1, 1]) or np.array_equal(_test_layer.backward(np.ones(5)), [0, 0, 1, 1, 1]), f\"backward expected {[0, 0, 1, 1, 1]} or {[0, 0, 0, 1, 1]}, but got {_test_layer.backward(np.ones(5))}\"\n",
    "\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer(Layer):      \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the softmax layer.\"\"\"\n",
    "        self.input = x # Store input\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Compute output\n",
    "        \n",
    "        x = x - np.max(x)\n",
    "        exp_x = np.exp(x)\n",
    "        sum_exp_x = np.sum(exp_x)\n",
    "        x = exp_x/sum_exp_x\n",
    "        \n",
    "        \n",
    "        # ********************\n",
    "        \n",
    "        self.output = x # Store output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "_test_layer = SoftmaxLayer()\n",
    "\n",
    "# Test forward pass\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward([[1, 1]]), [[0.5, 0.5]])\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward([[1, 2, 3]]), [[0.09003057, 0.24472847, 0.66524096]])\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward([[0, 1, 2]]), [[0.09003057, 0.24472847, 0.66524096]])\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward([[0]]), [[1]])\n",
    "np.testing.assert_array_almost_equal(_test_layer.forward([[0, 10, 100, 1e6]]), [[0, 0, 0, 1]], err_msg=\"Overflow\")\n",
    "\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Completing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forwar pass through the entire network.\"\"\"\n",
    "        # ********************\n",
    "        # TODO Compute output\n",
    "        # x = ...\n",
    "        \n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        \n",
    "        \n",
    "        # ********************\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Backward pass through the entire network.\"\"\"\n",
    "        # ********************\n",
    "        # TODO Back propagate gradients through all layers\n",
    "        \n",
    "        for l in reversed(self.layers):\n",
    "            gradient = l.backward(gradient)\n",
    "        \n",
    "        # ********************\n",
    "\n",
    "    def train(self, x, target, learn_rate):\n",
    "        \"\"\"Train one batch.\"\"\"\n",
    "        gradient = self.forward(x) - target  # Assumes quadratic loss function\n",
    "        self.backward(gradient)  # Backprop\n",
    "        \n",
    "        # Update weights in all layers\n",
    "        for layer in self.layers:\n",
    "            layer.update(learn_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "_test_net = FeedForwardNet([LinearLayer(4, 10, seed=0),\n",
    "                      ReLULayer(),\n",
    "                      LinearLayer(10, 3, seed=1),\n",
    "                      SoftmaxLayer(),\n",
    "                     ])\n",
    "np.testing.assert_array_almost_equal(_test_net.forward([1,2,3,4]), [0.27760362, 0.64002456, 0.08237182])\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A2 - Autoencoder on MNIST\n",
    "For more information about MNIST, refer to the assignment PDF.\n",
    "\n",
    "The next cell will download the dataset and show some examples images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='data/', download=True, transform=transforms.ToTensor())\n",
    "mnist_test = np.array([x.numpy() for x, y in torchvision.datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor())]).reshape(-1, 28, 28)\n",
    "data_loader = torch.utils.data.DataLoader(mnist, batch_size=32, shuffle=True)\n",
    "\n",
    "# Show examples\n",
    "plt.figure(figsize=(16,2))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    \n",
    "    # Choose first example with the corresponding digit\n",
    "    example = next(x for x, y in mnist if y == i).reshape(28, 28)\n",
    "    plt.imshow(example, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Build the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build autoencoder\n",
    "autoencoder = FeedForwardNet([LinearLayer(784,175), ReLULayer(),LinearLayer(175,16), LinearLayer(16,300), ReLULayer(), LinearLayer(300,784)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Code\n",
    "assert all(type(l) in [LinearLayer, ReLULayer] for l in autoencoder.layers), \"Only use Linear and ReLu layers\"\n",
    "assert autoencoder.layers[0].w.shape[0] == 784, \"Wrong size of input\"\n",
    "assert autoencoder.layers[-1].w.shape[1] == 784, \"Wrong size of output\"\n",
    "assert min(l.w.shape[0] for l in autoencoder.layers if type(l) == LinearLayer) == 16, \"Wrong size of the bottleneck\"\n",
    "\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### b) Train the Autoencoder\n",
    "\n",
    "The next step is to train the autoencoder on the MNIST dataset via minibatch gradient descend.\n",
    "\n",
    "Implement the training of a batch. \n",
    "The incoming batches of images have a shape of (batch_size, 28, 28). To process them in a linear layer, you first need to reshape them to (batch_size, 28 * 28).\n",
    "Then, use the `train` method of the implemented `FeedForwardNetwork` to train the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "learn_rate = 0.0004\n",
    "\n",
    "losses = np.empty((epochs, 2))\n",
    "with tqdm(range(epochs)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "        for batch, _ in data_loader:\n",
    "            # ********************\n",
    "            # TODO Reshape and train batch\n",
    "            # batch =\n",
    "            \n",
    "            batch = batch.numpy() #np.float64(...)\n",
    "            batch = batch.reshape(batch.shape[0],28*28) #batch.shape[0] -> -1\n",
    "            \n",
    "            autoencoder.train(batch, batch, learn_rate)\n",
    "            \n",
    "            \n",
    "            # ********************\n",
    "\n",
    "            running_loss += np.sum((autoencoder.layers[-1].output - batch)**2)\n",
    "        \n",
    "        # Log losses and update progress bar\n",
    "        train_loss = running_loss/len(mnist)\n",
    "        validation_loss = np.sum(np.mean((autoencoder.forward(mnist_test.reshape(-1, 28*28))-mnist_test.reshape(-1, 28*28))**2, axis=0))\n",
    "        losses[epoch] = [train_loss, validation_loss]\n",
    "        pbar.set_description(f\"Loss {train_loss:.02f}/{validation_loss:.02f}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2b.pickle\", \"wb\") as f:\n",
    "    pickle.dump(autoencoder, f)\n",
    "\n",
    "# Visualize losses\n",
    "losses = np.array(losses)\n",
    "plt.plot(np.arange(len(losses)), losses[:,0], label=\"train\")\n",
    "plt.plot(np.arange(len(losses)), losses[:,1], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2b-train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show reconstruction\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(16):\n",
    "    # Show image\n",
    "    plt.subplot(4,8,2*i+1)\n",
    "    plt.imshow(mnist[i][0].reshape((28,28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Show reconstruction\n",
    "    plt.subplot(4,8, 2*i+2)\n",
    "    plt.imshow(autoencoder.forward(mnist[i][0].reshape(28*28)).reshape(28,28), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2b-rec.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************\n",
    "# TODO Split the autoencoder at the bottleneck into encoder and decoder \n",
    "\n",
    "for l in autoencoder.layers:\n",
    "    if (type(l) == LinearLayer) and (l.w.shape[0] == 16):\n",
    "        t = autoencoder.layers.index(l)\n",
    "        \n",
    "encoder = FeedForwardNet(autoencoder.layers[:t])\n",
    "decoder = FeedForwardNet(autoencoder.layers[t:])\n",
    "\n",
    "# ********************\n",
    "\n",
    "#  Choose two images \n",
    "image_a = mnist[5][0]\n",
    "image_b = mnist[4][0]\n",
    "\n",
    "# Compute their latent representations\n",
    "latent_a = encoder.forward(image_a.reshape(28*28))\n",
    "latent_b = encoder.forward(image_b.reshape(28*28))\n",
    "\n",
    "\n",
    "steps=10\n",
    "plt.figure(figsize=(16,2))\n",
    "for i, f in enumerate(np.linspace(0, 1, steps)):\n",
    "    plt.subplot(1, steps, i+1)\n",
    "    \n",
    "    # ********************\n",
    "    # TODO Interpolate between latent_a and latent_b with the mixing factor f\n",
    "    \n",
    "    latent = (1-f)*latent_a + f*latent_b\n",
    "    \n",
    "    # ********************\n",
    "    \n",
    "    plt.imshow(decoder.forward(latent).reshape(28, 28), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2c.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Latent space sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std of latent states\n",
    "latent_space = encoder.forward(mnist_test.reshape(-1, 28*28))\n",
    "latent_space_mean = np.mean(latent_space, axis=0)\n",
    "latent_space_std = np.std(latent_space, axis=0)\n",
    "\n",
    "# Sample from latent distribution\n",
    "plt.figure(figsize=(16,2))\n",
    "for i, f in enumerate(np.linspace(0,1,steps)):\n",
    "    plt.subplot(1, steps, i+1)\n",
    "    \n",
    "    # ********************\n",
    "    # TODO Sample from the normal distribution with latent_space_mean and latent_space_std\n",
    "    \n",
    "    latent = np.random.normal(latent_space_mean,latent_space_std)\n",
    "\n",
    "    # ********************\n",
    "    \n",
    "    plt.imshow(decoder.forward(latent).reshape(28, 28), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
